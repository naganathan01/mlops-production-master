trigger:
  schedules:
  - cron: "0 2 * * 1"  # Weekly on Monday at 2 AM
    displayName: Weekly model retraining
    branches:
      include:
      - main
    always: true

variables:
  pythonVersion: '3.9'
  
stages:
- stage: DataValidation
  displayName: 'Data Quality Check'
  jobs:
  - job: ValidateData
    displayName: 'Validate Training Data'
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: $(pythonVersion)
    
    - script: |
        pip install -r requirements.txt
        python src/data/validate_data.py --data-path $(DATA_PATH) --output validation_report.json
      displayName: 'Run data validation'
      env:
        DATA_PATH: $(TRAINING_DATA_PATH)
    
    - script: |
        python -c "
        import json
        with open('validation_report.json') as f:
            report = json.load(f)
        if not report['data_quality']['passed']:
            print('Data quality checks failed!')
            exit(1)
        print('Data quality checks passed')
        "
      displayName: 'Check data quality results'

- stage: ModelTraining
  displayName: 'Train New Model'
  dependsOn: DataValidation
  condition: succeeded()
  jobs:
  - job: TrainModel
    displayName: 'Train and Evaluate Model'
    timeoutInMinutes: 120
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: $(pythonVersion)
    
    - script: |
        pip install -r requirements.txt
      displayName: 'Install dependencies'
    
    - script: |
        export MLFLOW_TRACKING_URI=$(MLFLOW_TRACKING_URI)
        python src/model/train.py \
          --data-path $(TRAINING_DATA_PATH) \
          --experiment-name "production-model-$(Build.BuildId)" \
          --model-name $(MODEL_NAME)
      displayName: 'Train new model'
      env:
        MLFLOW_TRACKING_URI: $(MLFLOW_TRACKING_URI)
    
    - script: |
        python src/model/evaluate.py \
          --model-uri "runs:/$(MLFLOW_RUN_ID)/model" \
          --test-data $(TEST_DATA_PATH) \
          --baseline-metrics baseline_metrics.json \
          --output-report evaluation_report.json
      displayName: 'Evaluate model performance'
    
    - script: |
        python -c "
        import json
        with open('evaluation_report.json') as f:
            report = json.load(f)
        
        # Check if new model beats baseline
        current_accuracy = report['metrics']['accuracy']
        baseline_accuracy = report['baseline']['accuracy']
        
        if current_accuracy < baseline_accuracy * 0.95:  # Allow 5% degradation
            print(f'Model performance degraded: {current_accuracy} vs {baseline_accuracy}')
            exit(1)
        
        print(f'Model performance acceptable: {current_accuracy} vs {baseline_accuracy}')
        print('##vso[task.setvariable variable=ModelPromotionRequired]true')
        "
      displayName: 'Compare with baseline'

- stage: ModelPromotion
  displayName: 'Promote Model'
  dependsOn: ModelTraining
  condition: and(succeeded(), eq(variables['ModelPromotionRequired'], 'true'))
  jobs:
  - deployment: PromoteModel
    displayName: 'Promote to Staging'
    environment: 'model-registry'
    strategy:
      runOnce:
        deploy:
          steps:
          - script: |
              python src/utils/promote_model.py \
                --run-id $(MLFLOW_RUN_ID) \
                --model-name $(MODEL_NAME) \
                --stage Staging \
                --description "Auto-retrained model from pipeline $(Build.BuildId)"
            displayName: 'Promote model to Staging'
          
          - script: |
              # Trigger integration tests
              curl -X POST \
                -H "Authorization: Bearer $(SYSTEM_ACCESSTOKEN)" \
                -H "Content-Type: application/json" \
                -d '{"definition": {"id": $(INTEGRATION_TEST_PIPELINE_ID)}, "parameters": {"modelVersion": "Staging"}}' \
                "$(System.CollectionUri)$(System.TeamProject)/_apis/build/builds?api-version=6.0"
            displayName: 'Trigger integration tests'